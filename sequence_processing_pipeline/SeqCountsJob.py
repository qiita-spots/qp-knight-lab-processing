from collections import defaultdict
from .Job import Job, KISSLoader
from .PipelineError import JobFailedError
from glob import glob
from jinja2 import Environment
from metapool import load_sample_sheet
from os import walk
from os.path import join, split
import logging
import pandas as pd
from sequence_processing_pipeline.util import determine_orientation


logging.basicConfig(level=logging.DEBUG)


class SeqCountsJob(Job):
    def __init__(self, run_dir, output_path, queue_name,
                 node_count, wall_time_limit, jmem, modules_to_load,
                 qiita_job_id, max_array_length, files_to_count_path,
                 sample_sheet_path, cores_per_task=4):
        """
        ConvertJob provides a convenient way to run bcl-convert or bcl2fastq
        on a directory BCL files to generate Fastq files.
        :param run_dir: The 'run' directory that contains BCL files.
        :param output_path: Path where all pipeline-generated files live.
        :param queue_name: The name of the Torque queue to use for processing.
        :param node_count: The number of nodes to request.
        :param wall_time_limit: A hard time limit (in min) to bound processing.
        :param jmem: String representing total memory limit for entire job.
        :param modules_to_load: A list of Linux module names to load
        :param qiita_job_id: identify Torque jobs using qiita_job_id
        :param max_array_length: A hard-limit for array-sizes
        :param files_to_count_path: A path to a list of file-paths to count.
        :param sample_sheet_path: A path to the sample-sheet.
        :param cores_per_task: (Optional) # of CPU cores per node to request.
        """
        super().__init__(run_dir,
                         output_path,
                         'SeqCountsJob',
                         [],
                         max_array_length,
                         modules_to_load=modules_to_load)

        self.queue_name = queue_name
        self.node_count = node_count
        self.wall_time_limit = wall_time_limit
        self.cores_per_task = cores_per_task

        # raise an Error if jmem is not a valid floating point value.
        self.jmem = str(int(jmem))
        self.qiita_job_id = qiita_job_id
        self.jinja_env = Environment(loader=KISSLoader('templates'))

        self.job_name = (f"seq_counts_{self.qiita_job_id}")
        self.files_to_count_path = files_to_count_path
        self.sample_sheet_path = sample_sheet_path

        with open(self.files_to_count_path, 'r') as f:
            lines = f.readlines()
            lines = [x.strip() for x in lines]
            lines = [x for x in lines if x != '']
            self.file_count = len(lines)

    def run(self, callback=None):
        job_script_path = self._generate_job_script()
        params = ['--parsable',
                  f'-J {self.job_name}',
                  f'--array 1-{self.file_count}']
        try:
            self.job_info = self.submit_job(job_script_path,
                                            job_parameters=' '.join(params),
                                            exec_from=None,
                                            callback=callback)

            logging.debug(f'SeqCountsJob Job Info: {self.job_info}')
        except JobFailedError as e:
            # When a job has failed, parse the logs generated by this specific
            # job to return a more descriptive message to the user.
            info = self.parse_logs()
            # prepend just the message component of the Error.
            info.insert(0, str(e))
            raise JobFailedError('\n'.join(info))

        self.mark_job_completed()

        self._aggregate_counts(self.sample_sheet_path)

        self.mark_post_processing_completed()

        logging.debug(f'SeqCountJob {self.job_info["job_id"]} completed')

    def _generate_job_script(self):
        job_script_path = join(self.output_path, "seq_counts.sbatch")
        template = self.jinja_env.get_template("seq_counts.sbatch")

        #  got to make files_to_count.txt and put it in the output directory

        with open(job_script_path, mode="w", encoding="utf-8") as f:
            f.write(template.render({
                "job_name": "seq_counts",
                "wall_time_limit": self.wall_time_limit,
                "mem_in_gb": self.jmem,
                "node_count": self.node_count,
                "cores_per_task": self.cores_per_task,
                "queue_name": self.queue_name,
                "file_count": self.file_count,
                "files_to_count_path": self.files_to_count_path,
                "output_path": self.output_path
            }))

        return job_script_path

    def parse_logs(self):
        # overrides Job.parse_logs() w/tailored parse for specific logs.
        files = sorted(glob(join(self.log_path, '*.err')))
        msgs = []

        for some_file in files:
            with open(some_file, 'r') as f:
                msgs += [line for line in f.readlines()
                         if line.startswith("[E::stk_size]")]

        return [msg.strip() for msg in msgs]

    def _aggregate_counts_by_file(self):
        # aggregates sequence & bp counts from a directory of log files.

        def extract_metadata(log_output_file_path):
            """
            extracts sequence & bp counts from individual log files.
            """
            with open(log_output_file_path, 'r') as f:
                lines = f.readlines()
                lines = [x.strip() for x in lines]
                if len(lines) != 2:
                    raise ValueError(
                        "error processing %s" % log_output_file_path)
                _dir, _file = split(lines[0])
                seq_counts, base_pairs = lines[1].split('\t')
                return _dir, _file, int(seq_counts), int(base_pairs)

        results = defaultdict(dict)

        for root, dirs, files in walk(self.log_path):
            for _file in files:
                if _file.endswith('.out'):
                    log_output_file = join(root, _file)
                    _dir, _file, seq_counts, base_pairs = \
                        extract_metadata(log_output_file)

                    results[_file] = {
                        'seq_counts': seq_counts,
                        'base_pairs': base_pairs
                    }

        return results

    def _aggregate_counts(self, sample_sheet_path):
        """
        Aggregate results by sample_ids and write to file.
        Args:
            sample_sheet_path:

        Returns: None
        """
        def get_metadata(sample_sheet_path):
            sheet = load_sample_sheet(sample_sheet_path)

            lanes = []

            if sheet.validate_and_scrub_sample_sheet():
                results = {}

            for sample in sheet.samples:
                sample_name = sample['Sample_Name']
                sample_id = sample['Sample_ID']
                lanes.append(sample['Lane'])
                results[sample_id] = sample_name

            lanes = list(set(lanes))

            if len(lanes) != 1:
                raise ValueError(
                    "More than one lane is declared in sample-sheet")

            return results, lanes[0]

        # get lane number and sample-sheet names and ids
        samples, lane = get_metadata(sample_sheet_path)

        # aggregate results by filename
        by_files = self._aggregate_counts_by_file()

        # the per-sample-fastqs will be named according to sample-id. Generate
        # a list of the sample-ids defined in the sample-sheet and sort them
        # from longest to shortest. This allows us to match sample-ids to
        # files correctly even when some sample-ids are subsets of longer
        # sample-ids e.g.: 'T_LS_7_15_15B_SRE' and 'T_LS_7_15_15B'. This is
        # important as SeqCounts is intended to count directories of fastq
        # files that don't necessarily obey the standard Illumina naming
        # convention e.g: samplename_S1_L001_R1_001.fastq.gz.
        sample_ids = list(samples.keys())
        sample_ids.sort(reverse=True, key=len)

        results = defaultdict(list)

        # generate a list of file names to associate with sample-ids.
        # only count forward and reverse reads. don't count I? or any other
        # type of file present.
        file_names = [x for x in list(by_files.keys()) if
                      determine_orientation(x) in ['R1', 'R2']]

        for sample_id in sample_ids:
            found = [x for x in file_names if x.startswith(sample_id)]

            if len(found) == 0:
                # zero file matches for a sample_id means that a per-sample
                # fastq file was not generated at the stage referenced by the
                # paths in self.files_to_count_path. For example, a per-sample
                # fastq file may not have been generated by bcl-convert for a
                # particular sample, or the filtered sample may be of zero-
                # length. These things are normal operation and any error is
                # going to be logged by those Job() objects. It's okay that
                # a match wasn't found for a given sample_id defined in the
                # sample-sheet.
                continue

            if len(found) != 2:
                # Raise an error if more or less than two matches are found
                # for a given sample-id because this our output must be the
                # total sequence count for forward and reverse reads.
                raise ValueError("Multiple file matches for sample-id "
                                 f"'{sample_id}' found: {found}")

            # remove the found elements from the list of files so they're
            # not associated with additional sample-ids in a subsequent
            # loop iteration.
            file_names = list(set(file_names) - set(found))

            results[sample_id] = found

        # output the results in CSV format.
        sample_ids = []
        raw_reads_r1r2 = []
        lanes = []

        for sample_id in results:
            sample_ids.append(sample_id)
            found = results[sample_id]
            seq_counts = 0

            for _file in found:
                seq_counts += by_files[_file]['seq_counts']

            raw_reads_r1r2.append(seq_counts)
            lanes.append(lane)

        df = pd.DataFrame(data={'Sample_ID': sample_ids,
                                'raw_reads_r1r2': raw_reads_r1r2,
                                'Lane': lanes})

        df.set_index(['Sample_ID', 'Lane'], verify_integrity=True)

        # sort results into a predictable order for testing purposes
        df = df.sort_values(by='Sample_ID')

        result_path = join(self.output_path, 'SeqCounts.csv')
        df.to_csv(result_path, index=False, sep=",")

        return result_path
